{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from math import log2\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATSET = 'data'\n",
    "\n",
    "START_TRAIN_IMG_SIZE = 4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LR = 1e-3\n",
    "BATCH_SIZES = [256,256,128,64,32,16]# since when the image size increases, the computational cost also increases. So, we should decrease the bacth size\n",
    "CHANNELS_IMG = 3\n",
    "Z_DIM = 512\n",
    "W_DIM = 512\n",
    "IN_CHANNELS = 512\n",
    "LAMBDA_GP = 10 ##used for caclulating discriminator loss\n",
    "PROGRESSIVE_EPOCHS = [30] * len(BATCH_SIZES)#we can ensure that each stage of the model is adequately trained before moving on to the next stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(image_size): ##image_size is the output size of each layer\n",
    "    trainsform = transforms.Compose(\n",
    "        [transforms.Resize((image_size, image_size)),\n",
    "         transforms.ToTensor(), # c*h*w and value betwenn 0,1 (like a list of list)\n",
    "         transforms.RandomHorizontalFlip(p=0.5),\n",
    "         transforms.Normalize(\n",
    "            [0.5 for _ in range(CHANNELS_IMG)], #mean\n",
    "            [0.5 for _ in range(CHANNELS_IMG)], #sigma\n",
    "         )\n",
    "        ]\n",
    "    )\n",
    "    batch_size = BATCH_SIZES[int(log2(image_size/4))] ## inner part determines the number of resolution levels in the generator and discriminator\n",
    "    dataset=datasets.Flowers102(root=DATSET, split= 'train', transform=trainsform, download = True)\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    return loader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/102flowers.tgz to data/flowers-102/102flowers.tgz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0572fc001404068b2c4d774456a9e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/344862509 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/flowers-102/102flowers.tgz to data/flowers-102\n",
      "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/imagelabels.mat to data/flowers-102/imagelabels.mat\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2db273d7432448bb80894afc0b37365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/setid.mat to data/flowers-102/setid.mat\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944c6cb96e284ce0ab24f615c4ae020a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14989 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m             ax[k][kk]\u001b[39m.\u001b[39mimshow((cloth[ind]\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m0\u001b[39m)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     10\u001b[0m             ind \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m---> 11\u001b[0m check_loader() \n",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m, in \u001b[0;36mcheck_loader\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_loader\u001b[39m(): \u001b[39m# a simple visual check to make sure that the data loader is working correctly\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m     loader,_ \u001b[39m=\u001b[39m get_loader(\u001b[39m128\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m     cloth,_  \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(loader)) \u001b[39m#retrieves the next batch of data from a PyTorch data loader\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     _,ax     \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m3\u001b[39m,\u001b[39m3\u001b[39m,figsize\u001b[39m=\u001b[39m(\u001b[39m8\u001b[39m,\u001b[39m8\u001b[39m))\n",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m, in \u001b[0;36mget_loader\u001b[0;34m(image_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m trainsform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose(\n\u001b[1;32m      3\u001b[0m     [transforms\u001b[39m.\u001b[39mResize((image_size, image_size)),\n\u001b[1;32m      4\u001b[0m      transforms\u001b[39m.\u001b[39mToTensor(), \u001b[39m# c*h*w and value betwenn 0,1 (like a list of list)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     ]\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m batch_size \u001b[39m=\u001b[39m BATCH_SIZES[\u001b[39mint\u001b[39m(log2(image_size\u001b[39m/\u001b[39m\u001b[39m4\u001b[39m))] \u001b[39m## inner part determines the number of resolution levels in the generator and discriminator\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m dataset\u001b[39m=\u001b[39mdatasets\u001b[39m.\u001b[39;49mFlowers102(root\u001b[39m=\u001b[39;49mDATSET, split\u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m, transform\u001b[39m=\u001b[39;49mtrainsform, download \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     15\u001b[0m loader \u001b[39m=\u001b[39m DataLoader(\n\u001b[1;32m     16\u001b[0m     dataset,\n\u001b[1;32m     17\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m     18\u001b[0m     shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[39mreturn\u001b[39;00m loader, dataset\n",
      "File \u001b[0;32m~/Desktop/cgip_gan_2023/.venv/lib/python3.10/site-packages/torchvision/datasets/flowers102.py:62\u001b[0m, in \u001b[0;36mFlowers102.__init__\u001b[0;34m(self, root, split, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_integrity():\n\u001b[1;32m     60\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mimport\u001b[39;00m loadmat\n\u001b[1;32m     64\u001b[0m set_ids \u001b[39m=\u001b[39m loadmat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_base_folder \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file_dict[\u001b[39m\"\u001b[39m\u001b[39msetid\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m], squeeze_me\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     65\u001b[0m image_ids \u001b[39m=\u001b[39m set_ids[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_splits_map[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split]]\u001b[39m.\u001b[39mtolist()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "def check_loader(): # a simple visual check to make sure that the data loader is working correctly\n",
    "    loader,_ = get_loader(128)\n",
    "    cloth,_  = next(iter(loader)) #retrieves the next batch of data from a PyTorch data loader\n",
    "    _,ax     = plt.subplots(3,3,figsize=(8,8))\n",
    "    plt.suptitle('Some real samples')\n",
    "    ind = 0\n",
    "    for k in range(3):\n",
    "        for kk in range(3):\n",
    "            ax[k][kk].imshow((cloth[ind].permute(1,2,0)+1)/2)\n",
    "            ind +=1\n",
    "check_loader() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noice apping network\n",
    "# AdaIN\n",
    "# Progressice growing\n",
    "factors = [1,1,1,1/2,1/4,1/8,1/16,1/32] # used to reduce 512 channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mWSLinear\u001b[39;00m(nn\u001b[39m.\u001b[39mModule): \u001b[39m#it is just one layer to scale weights\u001b[39;00m\n\u001b[1;32m      2\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m      3\u001b[0m         \u001b[39mself\u001b[39m, in_features, out_features\n\u001b[1;32m      4\u001b[0m     ):\n\u001b[1;32m      5\u001b[0m         \u001b[39msuper\u001b[39m(WSLinear,\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()                                                         \u001b[39m#what is the difference if i call super().__init__() ??\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class WSLinear(nn.Module): #it is just one layer to scale weights\n",
    "    def __init__(\n",
    "        self, in_features, out_features\n",
    "    ):\n",
    "        super().__init__()                                                       \n",
    "        \n",
    "        self.linear = nn.Linear(in_features, out_features)# linear transformaton on data y=X*W(^T)+b\n",
    "        self.scale  = (2/in_features) ** 0.5 ## scaling the weights of each layer according to the input size.\n",
    "        self.bias   = self.linear.bias\n",
    "        self.linear.bias = None #By setting the bias to None, the bias of the linear layer is removed and will not be used during the forward pass\n",
    "\n",
    "        nn.init.normal_(self.linear.weight)#values of W are drawn from a normal distribution with mean 0 and standard deviation 1.\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.linear(x * self.scale) + self.bias ##weight scaling technique involves removing the bias from the linear layer so add bias seperately\n",
    "    \n",
    "    # The output of this linear layer will be a set of scaling factors for each channel of the input tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mPixenNorm\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\u001b[39m#normalize features to be in the same range improving stability\u001b[39;00m\n\u001b[1;32m      2\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m      3\u001b[0m         \u001b[39msuper\u001b[39m(PixenNorm, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class PixenNorm(nn.Module):#normalize features to be in the same range improving stability\n",
    "    def __init__(self):\n",
    "        super(PixenNorm, self).__init__()\n",
    "        self.epsilon = 1e-8 ##to prevent division by zero\n",
    "    def forward(self,x ):\n",
    "        return x / torch.sqrt(torch.mean(x**2, dim=1, keepdim=True)+  self.epsilon)## Mean across rows since dim=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, z_dim, w_dim):\n",
    "        super().__init__()\n",
    "        self.mapping = nn.Sequential(\n",
    "            PixenNorm(),                                \n",
    "            WSLinear(z_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            WSLinear(w_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            WSLinear(w_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            WSLinear(w_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            WSLinear(w_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            WSLinear(w_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            WSLinear(w_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            WSLinear(w_dim, w_dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.mapping(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaIN(nn.Module):\n",
    "    def __init__(self, channels, w_dim):\n",
    "        super().__init__()\n",
    "        self.instance_norm = nn.InstanceNorm2d(channels)\n",
    "        self.style_scale   = WSLinear(w_dim, channels) #as ys to the size of channels\n",
    "        self.style_bias    = WSLinear(w_dim, channels) #as yb to the size of channels\n",
    "\n",
    "    def forward(self,x,w): #w is a learnable style vector\n",
    "        x = self.instance_norm(x) \n",
    "        style_scale = self.style_scale(w).unsqueeze(2).unsqueeze(3)#add dimention to w to be in the size of x\n",
    "        style_bias  = self.style_bias(w).unsqueeze(2).unsqueeze(3)\n",
    "        return style_scale * x + style_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class injectNoise(nn.Module):#introduce stochasticity and prevent overfitting during training\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.zeros(1,channels,1,1))\n",
    "\n",
    "    def forward(self, x): #size(numofbatch,channel,h,w)\n",
    "        noise = torch.randn((x.shape[0], 1, x.shape[2], x.shape[3]), device = x.device)\n",
    "        \n",
    "        B=self.weight *noise  \n",
    "        return x + B                                                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSConv2d(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "    ):\n",
    "        super(WSConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.scale = (2 / (in_channels * (kernel_size ** 2))) ** 0.5\n",
    "        self.bias = self.conv.bias\n",
    "        self.conv.bias = None\n",
    "\n",
    "        # initialize conv layer\n",
    "        nn.init.normal_(self.conv.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x * self.scale) + self.bias.view(1, self.bias.shape[0], 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, w_dim):\n",
    "        super(GenBlock, self).__init__()\n",
    "        self.conv1 = WSConv2d(in_channel, out_channel)\n",
    "        self.conv2 = WSConv2d(out_channel, out_channel)\n",
    "        self.leaky = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.inject_noise1 = injectNoise(out_channel)\n",
    "        self.inject_noise2 = injectNoise(out_channel)\n",
    "        self.adain1 = AdaIN(out_channel, w_dim)\n",
    "        self.adain2 = AdaIN(out_channel, w_dim)\n",
    "    def forward(self, x,w):\n",
    "        x = self.adain1(self.leaky(self.inject_noise1(self.conv1(x))), w)\n",
    "        x = self.adain2(self.leaky(self.inject_noise2(self.conv2(x))), w)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, w_dim, in_channels, img_channels=3):\n",
    "        super().__init__()\n",
    "        self.starting_cte = nn.Parameter(torch.ones(1, in_channels, 4,4))\n",
    "        self.map = MappingNetwork(z_dim, w_dim)\n",
    "        self.initial_adain1 = AdaIN(in_channels, w_dim)\n",
    "        self.initial_adain2 = AdaIN(in_channels, w_dim)\n",
    "        self.initial_noise1 = injectNoise(in_channels)\n",
    "        self.initial_noise2 = injectNoise(in_channels)\n",
    "        self.initial_conv   = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.leaky          = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "        self.initial_rgb    = WSConv2d(\n",
    "            in_channels, img_channels, kernel_size = 1, stride=1, padding=0\n",
    "        )\n",
    "        self.prog_blocks, self.rgb_layers = (\n",
    "            nn.ModuleList([]), #used to keep iterable modules  \n",
    "            nn.ModuleList([self.initial_rgb])\n",
    "        )\n",
    "\n",
    "        for i in range(len(factors)-1):\n",
    "            conv_in_c  = int(in_channels * factors[i])                                             \n",
    "            conv_out_c = int(in_channels * factors[i+1])\n",
    "            self.prog_blocks.append(GenBlock(conv_in_c, conv_out_c, w_dim))\n",
    "            self.rgb_layers.append(WSConv2d(conv_out_c, img_channels, kernel_size = 1, stride=1, padding=0))# in this part the chennel of output will be set to 3. it gets 512/16 as input ok\n",
    "        \n",
    "    def fade_in(self, alpha, upscaled, generated):                                                     #######important####### I think it needs to upsample upscaled images by 2 to be on the same size with output???\n",
    "        return torch.tanh(alpha * generated + (1-alpha ) * upscaled) #tanh is  an activation function\n",
    "\n",
    "    def forward(self, noise, alpha, steps):\n",
    "        w = self.map(noise)\n",
    "        x = self.initial_adain1(self.initial_noise1(self.starting_cte),w)                                 #### to remove mapping network we coud feed noise directrly instead of w, right???\n",
    "        x = self.initial_conv(x)\n",
    "        out = self.initial_adain2(self.leaky(self.initial_noise2(x)), w)\n",
    "\n",
    "        if steps == 0:\n",
    "            return self.initial_rgb(x)\n",
    "        \n",
    "        for step in range(steps):\n",
    "            upscaled = F.interpolate(out, scale_factor=2, mode = 'bilinear')\n",
    "            out      = self.prog_blocks[step](upscaled,w)\n",
    "\n",
    "        final_upscaled = self.rgb_layers[steps-1](upscaled)\n",
    "        final_out      = self.rgb_layers[steps](out)\n",
    "\n",
    "        return self.fade_in(alpha, final_upscaled, final_out)#applied at the end to blend the outputs of the previous and current step, produce smooth transition between resolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = WSConv2d(in_channels, out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels, out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):                                                                                                                 ##it gets fack and the previous ouput, fade,calculate std, set of liniear layer on it...\n",
    "    def __init__(self, in_channels, img_channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.prog_blocks, self.rgb_layers = nn.ModuleList([]), nn.ModuleList([])\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "        # here we work back ways from factors because the discriminator\n",
    "        # should be mirrored from the generator. So the first prog_block and\n",
    "        # rgb layer we append will work for input size 1024x1024, then 512->256-> etc\n",
    "        for i in range(len(factors) - 1, 0, -1):\n",
    "            conv_in = int(in_channels * factors[i])\n",
    "            conv_out = int(in_channels * factors[i - 1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in, conv_out))\n",
    "            self.rgb_layers.append(\n",
    "                WSConv2d(img_channels, conv_in, kernel_size=1, stride=1, padding=0)\n",
    "            )\n",
    "\n",
    "        # perhaps confusing name \"initial_rgb\" this is just the RGB layer for 4x4 input size\n",
    "        # did this to \"mirror\" the generator initial_rgb\n",
    "        self.initial_rgb = WSConv2d(\n",
    "            img_channels, in_channels, kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "        self.rgb_layers.append(self.initial_rgb)\n",
    "        self.avg_pool = nn.AvgPool2d(\n",
    "            kernel_size=2, stride=2\n",
    "        )  # down sampling using avg pool                                                                                                #does it downsample real images?\n",
    "\n",
    "        # this is the block for 4x4 input size\n",
    "        self.final_block = nn.Sequential(                                                                                                     #???\n",
    "            # +1 to in_channels because we concatenate from MiniBatch std\n",
    "            WSConv2d(in_channels + 1, in_channels, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels, in_channels, kernel_size=4, padding=0, stride=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(\n",
    "                in_channels, 1, kernel_size=1, padding=0, stride=1\n",
    "            ),  # we use this instead of linear layer\n",
    "        )\n",
    "\n",
    "    def fade_in(self, alpha, downscaled, out):\n",
    "        \"\"\"Used to fade in downscaled using avg pooling and output from CNN\"\"\"\n",
    "        # alpha should be scalar within [0, 1], and upscale.shape == generated.shape\n",
    "        return alpha * out + (1 - alpha) * downscaled\n",
    "\n",
    "    def minibatch_std(self, x):#calculates the standard deviation across the batch for each channel\n",
    "        batch_statistics = (\n",
    "            torch.std(x, dim=0).mean().repeat(x.shape[0], 1, x.shape[2], x.shape[3])\n",
    "        )\n",
    "        # we take the std for each example (across all channels, and pixels) then we repeat it\n",
    "        # for a single channel and concatenate it with the image. In this way the discriminator\n",
    "        # will get information about the variation in the batch/image\n",
    "        return torch.cat([x, batch_statistics], dim=1) \n",
    "\n",
    "    def forward(self, x, alpha, steps):\n",
    "        # where we should start in the list of prog_blocks, maybe a bit confusing but\n",
    "        # the last is for the 4x4. So example let's say steps=1, then we should start\n",
    "        # at the second to last because input_size will be 8x8. If steps==0 we just\n",
    "        # use the final block\n",
    "        cur_step = len(self.prog_blocks) - steps\n",
    "\n",
    "        # convert from rgb as initial step, this will depend on\n",
    "        # the image size (each will have it's on rgb layer)\n",
    "        out = self.leaky(self.rgb_layers[cur_step](x))\n",
    "\n",
    "        if steps == 0:  # i.e, image is 4x4\n",
    "            out = self.minibatch_std(out)\n",
    "            return self.final_block(out).view(out.shape[0], -1)\n",
    "\n",
    "        # because prog_blocks might change the channels, for down scale we use rgb_layer\n",
    "        # from previous/smaller size which in our case correlates to +1 in the indexing\n",
    "        downscaled = self.leaky(self.rgb_layers[cur_step + 1](self.avg_pool(x)))\n",
    "        out = self.avg_pool(self.prog_blocks[cur_step](out))\n",
    "\n",
    "        # the fade_in is done first between the downscaled and the input\n",
    "        # this is opposite from the generator\n",
    "        out = self.fade_in(alpha, downscaled, out)\n",
    "\n",
    "        for step in range(cur_step + 1, len(self.prog_blocks)):\n",
    "            out = self.prog_blocks[step](out)\n",
    "            out = self.avg_pool(out)\n",
    "\n",
    "        out = self.minibatch_std(out)\n",
    "        return self.final_block(out).view(out.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_examples(gen, steps, n=100):\n",
    "\n",
    "    gen.eval()\n",
    "    alpha = 1.0\n",
    "    for i in range(n):\n",
    "        with torch.no_grad():\n",
    "            noise = torch.randn(1, Z_DIM).to(DEVICE)\n",
    "            img = gen(noise, alpha, steps)\n",
    "            if not os.path.exists(f'saved_examples/step{steps}'):\n",
    "                os.makedirs(f'saved_examples/step{steps}')\n",
    "            save_image(img*0.5+0.5, f\"saved_examples/step{steps}/img_{i}.png\")\n",
    "    gen.train()\n",
    "    \n",
    "    #in train mode it updates parameters while in eval it does not compute gradient and it is used to evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real, fake, alpha, train_step, device=\"cpu\"):\n",
    "    BATCH_SIZE, C, H, W = real.shape\n",
    "    beta = torch.rand((BATCH_SIZE, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * beta + fake.detach() * (1 - beta)\n",
    "    interpolated_images.requires_grad_(True)\n",
    "\n",
    "    # Calculate critic scores\n",
    "    mixed_scores = critic(interpolated_images, alpha, train_step)\n",
    " \n",
    "    # Take the gradient of the scores with respect to the images\n",
    "    gradient = torch.autograd.grad(#Computes and returns the sum of gradients of outputs with respect to the inputs(dl/di?)\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)#||a||\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    critic,\n",
    "    gen,\n",
    "    loader,\n",
    "    dataset,\n",
    "    step,\n",
    "    alpha,\n",
    "    opt_critic,\n",
    "    opt_gen\n",
    "):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "\n",
    "    for batch_idx, (real, _) in enumerate(loop):\n",
    "        real = real.to(DEVICE)\n",
    "        cur_batch_size = real.shape[0]\n",
    "        noise = torch.randn(cur_batch_size, Z_DIM).to(DEVICE)\n",
    "        fake  = gen(noise, alpha, step)\n",
    "        critic_real = critic(real, alpha, step)\n",
    "        critic_fake = critic(fake.detach(), alpha, step)\n",
    "        gp = gradient_penalty(critic, real, fake, alpha, step, DEVICE)\n",
    "        loss_critic = (#Compute the loss for the critic model using the Wasserstein distance \n",
    "            -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
    "            + LAMBDA_GP * gp\n",
    "            + (0.001) * torch.mean(critic_real ** 2)\n",
    "        )\n",
    "\n",
    "        critic.zero_grad()\n",
    "        loss_critic.backward()\n",
    "        opt_critic.step()#Update the critic model parameters\n",
    "\n",
    "        gen_fake = critic(fake, alpha, step)\n",
    "        loss_gen = -torch.mean(gen_fake)#Compute the loss for the generator model using the negative critic loss \n",
    "\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        alpha += cur_batch_size / (#Adjust the alpha value based on the number of processed images and the total number of images in the datasetAdjust the alpha value based on the number of processed images and the total number of images in the dataset\n",
    "            PROGRESSIVE_EPOCHS[step] * 0.5 * len(dataset)\n",
    "        )\n",
    "        alpha = min(alpha,1)\n",
    "\n",
    "\n",
    "        loop.set_postfix(\n",
    "            gp = gp.item(),\n",
    "            loss_critic = loss_critic.item()\n",
    "        )\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(\n",
    "    Z_DIM, W_DIM, IN_CHANNELS, CHANNELS_IMG\n",
    ").to(DEVICE)\n",
    "critic = Discriminator(IN_CHANNELS, CHANNELS_IMG).to(DEVICE)\n",
    "\n",
    "#Implements Adam algorithm.\n",
    "\n",
    "opt_gen = optim.Adam([{'params': [param for name, param in gen.named_parameters() if 'map' not in name]},\n",
    "                     {'params': gen.map.parameters(), 'lr': 1e-5}], lr=LR, betas =(0.0, 0.99))\n",
    "opt_critic = optim.Adam(\n",
    "    critic.parameters(), lr= LR, betas =(0.0, 0.99)\n",
    ")\n",
    "\n",
    "gen.train()#training mode\n",
    "critic.train()\n",
    "step = int(log2(START_TRAIN_IMG_SIZE / 4))\n",
    "for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n",
    "    alpha = 1e-7\n",
    "    loader, dataset = get_loader(4*2**step)\n",
    "    print('Curent image size: '+str(4*2**step))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch [{epoch + 1}/ {num_epochs}')\n",
    "        alpha = train_fn(\n",
    "            critic, gen, loader, dataset, step, alpha, opt_critic, opt_gen\n",
    "        )\n",
    "    generate_examples(gen, step)\n",
    "    step +=1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
